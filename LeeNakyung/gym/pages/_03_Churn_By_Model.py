


import streamlit as st
import pandas as pd
import numpy as np
import joblib
import torch
import torch.nn as nn
import pickle

# ====================== DNN í´ë˜ìŠ¤ ì •ì˜ ======================
class DNN(nn.Module):
    def __init__(self, Cin):
        super().__init__()
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.4)
        self.linear1 = nn.Linear(Cin, 32)
        self.linear2 = nn.Linear(32, 128)
        self.linear3 = nn.Linear(128, 16)
        self.fc = nn.Linear(16, 1)
    def forward(self, x):
        out = self.linear1(x)
        out = self.relu(out)
        out = self.linear2(out)
        out = self.relu(out)
        out = self.linear3(out)
        out = self.relu(out)
        out = self.dropout(out)
        out = self.fc(out)
        return out

# ====================== íŒŒì¼ ê²½ë¡œ ============================
MODEL_DIR = "../health/ml_without_phone/project/models/"
MODEL_FILES = {
    'Logistic Regression': MODEL_DIR + 'LogisticRegression.pkl',
    'Random Forest': MODEL_DIR + 'RandomForestClassifier.pkl',
    'Decision Tree': MODEL_DIR + 'DecisionTreeClassifier.pkl',
    'KNN': MODEL_DIR + 'KNeighborsClassifier.pkl',
    'SVC': MODEL_DIR + 'SVC.pkl',
    'LightGBM': MODEL_DIR + 'LGBMClassifier.pkl',
    'XGBoost': MODEL_DIR + 'XGBClassifier.pkl',
    'MLP': MODEL_DIR + 'MLPClassifier.pkl',
    'DNN': MODEL_DIR + 'best_model.pth'
}
SCALER_FILES = {
    'Logistic Regression': MODEL_DIR + 'LogisticRegression_scaler.pkl',
    'Random Forest': MODEL_DIR + 'RandomForestClassifier_scaler.pkl',
    'Decision Tree': MODEL_DIR + 'DecisionTreeClassifier_scaler.pkl',
    'KNN': MODEL_DIR + 'KNeighborsClassifier_scaler.pkl',
    'SVC': MODEL_DIR + 'SVC_scaler.pkl',
    'LightGBM': MODEL_DIR + 'LGBMClassifier_scaler.pkl',
    'XGBoost': MODEL_DIR + 'XGBClassifier_scaler.pkl',
    'MLP': MODEL_DIR + 'MLPClassifier_scaler.pkl',
    'DNN': MODEL_DIR + 'DNN_scaler.pkl'
}
KOREAN_DATA_PATH = "../health/ml_without_phone/splited_data/gym_test_korean.csv"
MODEL_INPUT_PATH = "../health/ml_without_phone/splited_data/gym_test_for_model.csv"

# ====================== í•¨ìˆ˜ ì •ì˜ ============================
def load_korean_data():
    return pd.read_csv(KOREAN_DATA_PATH, encoding='utf-8-sig')

def load_model_input_data():
    return pd.read_csv(MODEL_INPUT_PATH)

def load_model(path):
    try:
        return joblib.load(path)
    except Exception as e:
        raise e

def load_scaler(path):
    try:
        return joblib.load(path)
    except Exception as e:
        # DNN ìŠ¤ì¼€ì¼ëŸ¬ëŠ” pickleë¡œ ì €ì¥í–ˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ ì‹œë„
        with open(path, 'rb') as f:
            return pickle.load(f)

def check_model_probability_support(model):
    return hasattr(model, 'predict_proba')

# ====================== Streamlit UI ========================
st.title("ğŸ‹ï¸â€â™‚ï¸ í—¬ìŠ¤ì¥ íšŒì› ì´íƒˆ ì˜ˆì¸¡")

selected_model = st.selectbox(
    "ğŸ¤– ëª¨ë¸ ì„ íƒ",
    list(MODEL_FILES.keys()),
    index=0
)

try:
    korean_df = load_korean_data()
    model_input_df = load_model_input_data()

    st.info(f"ğŸ“Š ë¡œë“œëœ ë°ì´í„°: {korean_df.shape[0]}ê°œ íšŒì›, {korean_df.shape[1]}ê°œ íŠ¹ì„±")

    churn_col = 'ì´íƒˆ ì—¬ë¶€'
    churn_counts = korean_df[churn_col].value_counts()
    col1, col2 = st.columns(2)
    with col1:
        st.metric("í˜„ì¬ ì´ìš© ì¤‘", churn_counts.get('ìœ ì§€', 0))
    with col2:
        st.metric("ì´íƒˆí•œ íšŒì›", churn_counts.get('ì´íƒˆ', 0))

    with st.spinner(f"{selected_model} ëª¨ë¸ ë¡œë”© ì¤‘..."):
        scaler = load_scaler(SCALER_FILES[selected_model])
        X_test = model_input_df.drop(columns=['Churn'])
        y_test = model_input_df['Churn']

        # DNN ë¶„ê¸°
        if selected_model == "DNN":
            X_scaled = scaler.transform(X_test)
            Cin = X_scaled.shape[1]
            model = DNN(Cin)
            model.load_state_dict(torch.load(MODEL_FILES["DNN"], map_location="cpu"))
            model.eval()
            with torch.no_grad():
                X_tensor = torch.tensor(X_scaled, dtype=torch.float32)
                logits = model(X_tensor)
                probs = torch.sigmoid(logits).numpy().flatten()
                y_pred = (probs >= 0.5).astype(int)
                churn_probs = probs
            supports_proba = True
        else:
            model = load_model(MODEL_FILES[selected_model])
            X_scaled = scaler.transform(X_test)
            y_pred = model.predict(X_scaled)
            supports_proba = check_model_probability_support(model)
            if supports_proba:
                churn_probs = model.predict_proba(X_scaled)[:, 1]
            else:
                churn_probs = np.full(len(X_test), np.nan)

    # ====== ì„±ëŠ¥ ì§€í‘œ ì¶œë ¥ ======
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ì •í™•ë„", f"{accuracy:.3f}")
    with col2:
        st.metric("ì •ë°€ë„", f"{precision:.3f}")
    with col3:
        st.metric("ì¬í˜„ìœ¨", f"{recall:.3f}")
    with col4:
        st.metric("F1 ì ìˆ˜", f"{f1:.3f}")

    # ====== ê²°ê³¼ í‘œ ë§Œë“¤ê¸° ======
    result_df = korean_df.copy()
    result_df['ì˜ˆì¸¡ ê²°ê³¼'] = ['ìœ ì§€' if pred == 0 else 'ì´íƒˆ' for pred in y_pred]
    result_df['ì˜ˆì¸¡ ì •í™•ë„'] = ['ì •í™•' if actual == pred else 'ì˜¤ë¥˜'
                           for actual, pred in zip(y_test, y_pred)]

    if supports_proba:
        result_df['ì´íƒˆ í™•ë¥ '] = np.where(
            result_df[churn_col] == 'ìœ ì§€',
            np.round(churn_probs, 4),
            ''
        )
    else:
        result_df['ì´íƒˆ í™•ë¥ '] = np.where(
            result_df[churn_col] == 'ìœ ì§€',
            'N/A (í™•ë¥  ë¯¸ì§€ì›)',
            ''
        )
# ğŸ’° ì›ë³¸ ë°ì´í„° ì»¬ëŸ¼ë“¤ì˜ ê³„ì‚°ì‹ ì„¤ëª…
    with st.expander("ğŸ’° ì›ë³¸ ë°ì´í„° ì»¬ëŸ¼ ê³„ì‚°ì‹"):
        st.write("""
        ### ğŸ’° í‰ê·  ì¶”ê°€ ìš”ê¸ˆ
        ```
        í‰ê· _ì¶”ê°€_ìš”ê¸ˆ = ì´_ì¶”ê°€_ì„œë¹„ìŠ¤_ë¹„ìš© / ì´_ì´ìš©_ê°œì›”ìˆ˜
        ```
        **í¬í•¨ ì„œë¹„ìŠ¤**: ê°œì¸íŠ¸ë ˆì´ë‹(PT), ë½ì»¤ëŒ€ì—¬, ë‹¨ë°±ì§ˆìŒë£Œ, ì‚¬ìš°ë‚˜, íŠ¹ë³„í”„ë¡œê·¸ë¨, íƒ€ì›”ì„œë¹„ìŠ¤

        **ì˜ˆì‹œ**: ì´ 120ë§Œì› Ã· 12ê°œì›” = **ì›”í‰ê·  10ë§Œì›**

        ---

        ### ğŸƒâ€â™€ï¸ ì „ì²´ ìˆ˜ì—… ì°¸ì—¬ ë¹ˆë„
        ```
        ì „ì²´_ìˆ˜ì—…_ì°¸ì—¬_ë¹ˆë„ = ì´_ìˆ˜ì—…_ì°¸ì—¬_íšŸìˆ˜ / ì´_ì´ìš©_ì£¼ìˆ˜
        ```
        **í¬í•¨ ìˆ˜ì—…**: ìš”ê°€, í•„ë¼í…ŒìŠ¤, ìŠ¤í”¼ë‹, ì—ì–´ë¡œë¹…, í¬ë¡œìŠ¤í•, ìˆ˜ì˜ê°•ìŠµ

        **ì˜ˆì‹œ**: ì´ 65íšŒ Ã· 24ì£¼ = **ì£¼ë‹¹ 2.71íšŒ**

        ---

        ### ğŸ“… ì´ë²ˆë‹¬ ìˆ˜ì—… ì°¸ì—¬ ë¹ˆë„
        ```
        ì´ë²ˆë‹¬_ìˆ˜ì—…_ì°¸ì—¬_ë¹ˆë„ = ì´ë²ˆë‹¬_ìˆ˜ì—…_ì°¸ì—¬_íšŸìˆ˜ / 4
        ```
        **ì¸¡ì • ëª©ì **: ìµœê·¼ í™œë™ íŒ¨í„´ íŒŒì•…, ì´íƒˆ ì¡°ê¸° ì‹ í˜¸ ê°ì§€

        **ì˜ˆì‹œ**: ì´ 6íšŒ Ã· 4ì£¼ = **ì£¼ë‹¹ 1.5íšŒ**

        ### âš ï¸ ì´íƒˆ ìœ„í—˜ ì‹ í˜¸
        ```
        ì´ë²ˆë‹¬_ë¹ˆë„ < ì „ì²´_ë¹ˆë„ Ã— 0.5  â†’  í™œë™ ê¸‰ê° ìœ„í—˜
        ì´ë²ˆë‹¬_ë¹ˆë„ = 0  â†’  í™œë™ ì¤‘ë‹¨ ê³ ìœ„í—˜
        ```
        """)

    # ë°ì´í„° í‘œì‹œ ì˜µì…˜
    st.write("### ğŸ“‹ ì˜ˆì¸¡ ê²°ê³¼ ìƒì„¸")

    show_option = st.radio(
        "í‘œì‹œí•  ë°ì´í„° ì„ íƒ:",
        ["ì „ì²´ ë°ì´í„°", "í˜„ì¬ ì´ìš© ì¤‘ì¸ íšŒì›ë§Œ", "ì´íƒˆí•œ íšŒì›ë§Œ", "ì˜ˆì¸¡ ì‹¤íŒ¨ ì¼€ì´ìŠ¤ë§Œ"]
    )

    if show_option == "ì „ì²´ ë°ì´í„°":
        display_df = result_df
    elif show_option == "í˜„ì¬ ì´ìš© ì¤‘ì¸ íšŒì›ë§Œ":
        display_df = result_df[result_df[churn_col] == 'ìœ ì§€']
    elif show_option == "ì´íƒˆí•œ íšŒì›ë§Œ":
        display_df = result_df[result_df[churn_col] == 'ì´íƒˆ']
    else:  # ì˜ˆì¸¡ ì‹¤íŒ¨ ì¼€ì´ìŠ¤
        display_df = result_df[result_df['ì˜ˆì¸¡ ì •í™•ë„'] == 'ì˜¤ë¥˜']

    # ì»¬ëŸ¼ ìˆœì„œ ì¬ì •ë ¬ - í•œê¸€ ì»¬ëŸ¼ë“¤ ë¨¼ì €, ê³„ì‚°ëœ ì»¬ëŸ¼ë“¤ì„ ì§€ì •ëœ ìˆœì„œë¡œ
    korean_feature_columns = [col for col in display_df.columns
                              if col not in ['ì˜ˆì¸¡ ê²°ê³¼', 'ì˜ˆì¸¡ ì •í™•ë„', 'ì´íƒˆ í™•ë¥ ', churn_col]]
    calculated_columns = ['ì˜ˆì¸¡ ê²°ê³¼', churn_col, 'ì˜ˆì¸¡ ì •í™•ë„', 'ì´íƒˆ í™•ë¥ ']
    display_columns = korean_feature_columns + calculated_columns

    st.dataframe(
        display_df[display_columns],
        use_container_width=True,
        height=400
    )

    # ê³ ìœ„í—˜ íšŒì› í•˜ì´ë¼ì´íŠ¸ (í™•ë¥  ì˜ˆì¸¡ ì§€ì›í•˜ëŠ” ëª¨ë¸ë§Œ)
    if show_option in ["ì „ì²´ ë°ì´í„°", "í˜„ì¬ ì´ìš© ì¤‘ì¸ íšŒì›ë§Œ"] and supports_proba:
        high_risk_threshold = st.slider("ê³ ìœ„í—˜ ì„ê³„ê°’ ì„¤ì •", 0.1, 0.9, 0.7, 0.1)

        # í˜„ì¬ ì´ìš© ì¤‘ì´ë©´ì„œ ì´íƒˆ í™•ë¥ ì´ ë†’ì€ íšŒì› ì°¾ê¸°
        high_risk_mask = (
                (result_df[churn_col] == 'ìœ ì§€') &
                (pd.to_numeric(result_df['ì´íƒˆ í™•ë¥ '], errors='coerce') >= high_risk_threshold)
        )
        high_risk_members = result_df[high_risk_mask]

        if len(high_risk_members) > 0:
            st.warning(f"âš ï¸ **ê³ ìœ„í—˜ íšŒì› {len(high_risk_members)}ëª… ë°œê²¬!** (ì´íƒˆ í™•ë¥  >= {high_risk_threshold})")
            st.dataframe(
                high_risk_members[display_columns],
                use_container_width=True
            )
        else:
            st.success(f"âœ… ì´íƒˆ í™•ë¥  {high_risk_threshold} ì´ìƒì¸ ê³ ìœ„í—˜ íšŒì›ì´ ì—†ìŠµë‹ˆë‹¤.")
    elif not supports_proba:
        st.info(f"â„¹ï¸ {selected_model} ëª¨ë¸ì€ í™•ë¥  ì˜ˆì¸¡ì„ ì§€ì›í•˜ì§€ ì•Šì•„ ê³ ìœ„í—˜ íšŒì› ë¶„ì„ì„ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ëª¨ë¸ë³„ ì¶”ê°€ ì •ë³´ í‘œì‹œ
    if selected_model == 'SVC':
        st.info("""
        ğŸ“ **SVC ëª¨ë¸ ì •ë³´:**
        - Support Vector ClassifierëŠ” ê¸°ë³¸ì ìœ¼ë¡œ í™•ë¥  ì˜ˆì¸¡ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        - ë¶„ë¥˜ ê²½ê³„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì •í™•í•œ ë¶„ë¥˜ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
        - ê³ ì°¨ì› ë°ì´í„°ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.
        """)

    # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
    csv_data = result_df.to_csv(index=False, encoding='utf-8-sig')
    st.download_button(
        label="ğŸ“¥ ì˜ˆì¸¡ ê²°ê³¼ CSV ë‹¤ìš´ë¡œë“œ",
        data=csv_data,
        file_name=f"churn_prediction_korean_{selected_model.lower().replace(' ', '_')}.csv",
        mime="text/csv"
    )

except FileNotFoundError as e:
    st.error(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    st.info("ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”:")
    st.code("""
# 1. í•œê¸€ ë°ì´í„°ì…‹ ìƒì„± ì½”ë“œë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”
# 2. gym_test_korean.csvì™€ gym_test_for_model.csv íŒŒì¼ì´ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”
    """)

except Exception as e:
    st.error(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    st.write("ìƒì„¸ ì˜¤ë¥˜ ì •ë³´:")
    st.code(str(e))